{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "### We chose to parse the dataset from Stanford as it has more information and characteristics than the one from Kaggle. The dataset from Kaggle is a subset of the one from Stanford, so we decided to use the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+--------------------+---+---+-----------------+--------+--------+------+------------+-----------+---------------+---------+--------------------+---------------+--------------+--------+----------------+----------------+------------------+----------------+--------------+--------------+-------------+------------+-------------+------------+------------+---------------+------------------------+-----------------------------+-----------------------------+-----------------------------+\n",
      "|raw_row_number|      date|    time|            location|lat|lng|      county_name|district|precinct|region|subject_race|subject_sex|officer_id_hash|     type|           violation|citation_issued|warning_issued| outcome|contraband_found|contraband_drugs|contraband_weapons|search_conducted|search_vehicle|  search_basis|vehicle_color|vehicle_make|vehicle_model|vehicle_type|vehicle_year|raw_HA_RACE_SEX|raw_HA_SEARCH_PC_boolean|raw_HA_SEARCH_CONCENT_boolean|raw_HA_INCIDTO_ARREST_boolean|raw_HA_VEHICLE_INVENT_boolean|\n",
      "+--------------+----------+--------+--------------------+---+---+-----------------+--------+--------+------+------------+-----------+---------------+---------+--------------------+---------------+--------------+--------+----------------+----------------+------------------+----------------+--------------+--------------+-------------+------------+-------------+------------+------------+---------------+------------------------+-----------------------------+-----------------------------+-----------------------------+\n",
      "|             1|2006-01-01|00:00:00|route: 0030, mile...| NA| NA|    Walker County|       C|      NA|     2|       white|     female|     b754c6abf4|vehicular|Drive On Improved...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             WF|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|             2|2006-01-01|00:00:00|route: 0207, mile...| NA| NA|  Hansford County|       B|      11|     5|       white|       male|     7621d63a65|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             WM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|             3|2006-01-01|00:00:00|route: 0105, mile...| NA| NA|Montgomery County|       C|      20|     2|    hispanic|       male|     2c0d24dbbd|vehicular|Open Container in...|           TRUE|         FALSE|citation|            TRUE|           FALSE|             FALSE|            TRUE|            NA|probable cause|           NA|          NA|           NA|          SV|          NA|             HM|                    TRUE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|             4|2006-01-01|00:00:00|route: 0010, mile...| NA| NA|  Chambers County|       B|      NA|     2|       white|       male|     1abde6b2cf|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          SV|          NA|             WM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|             5|2006-01-01|00:00:00|route: 1774, mile...| NA| NA|Montgomery County|       C|      50|     2|       white|       male|     3f1120d85a|vehicular|No/Improper Licen...|           TRUE|          TRUE|citation|            TRUE|           FALSE|             FALSE|            TRUE|            NA|probable cause|           NA|          NA|           NA|          PA|          NA|             WM|                    TRUE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|             8|2006-01-01|00:00:00|route: 0035, mile...| NA| NA|      Frio County|       B|      NA|     3|    hispanic|       male|     bb8d7daa5d|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             HM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|             9|2006-01-01|00:00:00|route: 0385, mile...| NA| NA|    Castro County|       B|      NA|     5|    hispanic|       male|     46512c3e98|vehicular|No/Improper Licen...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|             HM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            10|2006-01-01|00:00:00|route: 0511, mile...| NA| NA|   Cameron County|       A|      21|     8|    hispanic|       male|     d71bcea4a5|vehicular|Fail to Maintain ...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PV|          NA|             HM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            11|2006-01-01|00:00:00|route: 0020, mile...| NA| NA|  Eastland County|       B|      20|     4|    hispanic|       male|     bfaf4ea0b6|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             HM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            12|2006-01-01|00:00:00|route: 0020, mile...| NA| NA| Van Zandt County|       B|      NA|     1|       white|       male|     9c1f3a985a|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             WM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            13|2006-01-01|00:00:00|route: 0287, mile...| NA| NA|  Anderson County|       C|      40|     6|       black|     female|     85d044df3d|vehicular|Fail To Display D...|           TRUE|          TRUE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             BF|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            14|2006-01-01|00:00:00|          route: 134| NA| NA|   Haskell County|       A|      10|     5|    hispanic|       male|     5d6ea74869|vehicular|Minor Consume Alc...|           TRUE|         FALSE|citation|            TRUE|           FALSE|             FALSE|            TRUE|            NA|probable cause|           NA|          NA|           NA|          PU|          NA|             HM|                    TRUE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            16|2006-01-01|00:00:00|          route: 134| NA| NA|   Haskell County|       A|      00|     5|       white|       male|     5d6ea74869|vehicular|Minor Consume Alc...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|             WM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            17|2006-01-01|00:01:00|route: 0059, mile...| NA| NA|   Wharton County|       C|      20|     2|       white|       male|     41cda6005f|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             WM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            18|2006-01-01|00:01:00|route: 0021, mile...| NA| NA|   Bastrop County|       C|      NA|     6|       black|     female|     8b99d7010d|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             BF|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            19|2006-01-01|00:01:00|route: 0238, mile...| NA| NA|   Calhoun County|       A|      NA|     3|       black|       male|     92f60bd84f|vehicular|No/Improper Licen...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|             BM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            20|2006-01-01|00:01:00|route: 0281, mile...| NA| NA|   Hidalgo County|       A|      42|     8|       white|       male|     3d49aa825d|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|             WM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            21|2006-01-01|00:01:00|route: 0256, mile...| NA| NA|  Anderson County|       C|      NA|     6|       white|     female|     85d044df3d|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             WF|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            22|2006-01-01|00:01:00|route: 0410, mile...| NA| NA|     Bexar County|       B|      20|     3|    hispanic|     female|     083d70bcc5|vehicular|Ride, Not Secured...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|             HF|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "|            23|2006-01-01|00:01:00|     route: FRONTAGE| NA| NA|   Cameron County|       A|      NA|     8|    hispanic|       male|     9d1b7b13f1|vehicular|No/Non-Compliant ...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|             HM|                   FALSE|                        FALSE|                        FALSE|                        FALSE|\n",
      "+--------------+----------+--------+--------------------+---+---+-----------------+--------+--------+------+------------+-----------+---------------+---------+--------------------+---------------+--------------+--------+----------------+----------------+------------------+----------------+--------------+--------------+-------------+------------+-------------+------------+------------+---------------+------------------------+-----------------------------+-----------------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('DataFrame Optimization') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the DataFrame\n",
    "df = spark.read.format('csv').option('header', 'true').load('../data/tx_statewide_2020_04_01.csv')\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### how many rows are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19752786"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### how many null values are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1426"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show number NaN values in violation column\n",
    "df.filter(df['violation'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove columns with name starting with \"raw_\" as they are not useful for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+---+---+-----------------+--------+--------+------+------------+-----------+---------------+---------+--------------------+---------------+--------------+--------+----------------+----------------+------------------+----------------+--------------+--------------+-------------+------------+-------------+------------+------------+\n",
      "|      date|    time|            location|lat|lng|      county_name|district|precinct|region|subject_race|subject_sex|officer_id_hash|     type|           violation|citation_issued|warning_issued| outcome|contraband_found|contraband_drugs|contraband_weapons|search_conducted|search_vehicle|  search_basis|vehicle_color|vehicle_make|vehicle_model|vehicle_type|vehicle_year|\n",
      "+----------+--------+--------------------+---+---+-----------------+--------+--------+------+------------+-----------+---------------+---------+--------------------+---------------+--------------+--------+----------------+----------------+------------------+----------------+--------------+--------------+-------------+------------+-------------+------------+------------+\n",
      "|2006-01-01|00:00:00|route: 0030, mile...| NA| NA|    Walker County|       C|      NA|     2|       white|     female|     b754c6abf4|vehicular|Drive On Improved...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:00:00|route: 0207, mile...| NA| NA|  Hansford County|       B|      11|     5|       white|       male|     7621d63a65|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:00:00|route: 0105, mile...| NA| NA|Montgomery County|       C|      20|     2|    hispanic|       male|     2c0d24dbbd|vehicular|Open Container in...|           TRUE|         FALSE|citation|            TRUE|           FALSE|             FALSE|            TRUE|            NA|probable cause|           NA|          NA|           NA|          SV|          NA|\n",
      "|2006-01-01|00:00:00|route: 0010, mile...| NA| NA|  Chambers County|       B|      NA|     2|       white|       male|     1abde6b2cf|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          SV|          NA|\n",
      "|2006-01-01|00:00:00|route: 1774, mile...| NA| NA|Montgomery County|       C|      50|     2|       white|       male|     3f1120d85a|vehicular|No/Improper Licen...|           TRUE|          TRUE|citation|            TRUE|           FALSE|             FALSE|            TRUE|            NA|probable cause|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:00:00|route: 0035, mile...| NA| NA|      Frio County|       B|      NA|     3|    hispanic|       male|     bb8d7daa5d|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:00:00|route: 0385, mile...| NA| NA|    Castro County|       B|      NA|     5|    hispanic|       male|     46512c3e98|vehicular|No/Improper Licen...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|\n",
      "|2006-01-01|00:00:00|route: 0511, mile...| NA| NA|   Cameron County|       A|      21|     8|    hispanic|       male|     d71bcea4a5|vehicular|Fail to Maintain ...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PV|          NA|\n",
      "|2006-01-01|00:00:00|route: 0020, mile...| NA| NA|  Eastland County|       B|      20|     4|    hispanic|       male|     bfaf4ea0b6|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:00:00|route: 0020, mile...| NA| NA| Van Zandt County|       B|      NA|     1|       white|       male|     9c1f3a985a|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:00:00|route: 0287, mile...| NA| NA|  Anderson County|       C|      40|     6|       black|     female|     85d044df3d|vehicular|Fail To Display D...|           TRUE|          TRUE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:00:00|          route: 134| NA| NA|   Haskell County|       A|      10|     5|    hispanic|       male|     5d6ea74869|vehicular|Minor Consume Alc...|           TRUE|         FALSE|citation|            TRUE|           FALSE|             FALSE|            TRUE|            NA|probable cause|           NA|          NA|           NA|          PU|          NA|\n",
      "|2006-01-01|00:00:00|          route: 134| NA| NA|   Haskell County|       A|      00|     5|       white|       male|     5d6ea74869|vehicular|Minor Consume Alc...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|\n",
      "|2006-01-01|00:01:00|route: 0059, mile...| NA| NA|   Wharton County|       C|      20|     2|       white|       male|     41cda6005f|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:01:00|route: 0021, mile...| NA| NA|   Bastrop County|       C|      NA|     6|       black|     female|     8b99d7010d|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:01:00|route: 0238, mile...| NA| NA|   Calhoun County|       A|      NA|     3|       black|       male|     92f60bd84f|vehicular|No/Improper Licen...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|\n",
      "|2006-01-01|00:01:00|route: 0281, mile...| NA| NA|   Hidalgo County|       A|      42|     8|       white|       male|     3d49aa825d|vehicular|Speeding-10% or M...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|\n",
      "|2006-01-01|00:01:00|route: 0256, mile...| NA| NA|  Anderson County|       C|      NA|     6|       white|     female|     85d044df3d|vehicular|Speeding Over Lim...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "|2006-01-01|00:01:00|route: 0410, mile...| NA| NA|     Bexar County|       B|      20|     3|    hispanic|     female|     083d70bcc5|vehicular|Ride, Not Secured...|           TRUE|         FALSE|citation|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PU|          NA|\n",
      "|2006-01-01|00:01:00|     route: FRONTAGE| NA| NA|   Cameron County|       A|      NA|     8|    hispanic|       male|     9d1b7b13f1|vehicular|No/Non-Compliant ...|          FALSE|          TRUE| warning|              NA|              NA|                NA|           FALSE|         FALSE|            NA|           NA|          NA|           NA|          PA|          NA|\n",
      "+----------+--------+--------------------+---+---+-----------------+--------+--------+------+------------+-----------+---------------+---------+--------------------+---------------+--------------+--------+----------------+----------------+------------------+----------------+--------------+--------------+-------------+------------+-------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns with name starting with 'raw_'\n",
    "df = df.drop(*[col for col in df.columns if col.startswith('raw_')])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove columns with more than 50% of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "\n",
    "# Calculate the number of records in the DataFrame\n",
    "total_records = df.count()\n",
    "\n",
    "# Create a new DataFrame that counts the number of nulls, NaNs, or Nones in each column\n",
    "null_counts = df.select([count(when((col(c) == 'NA') | (col(c) == 'na') | isnan(c) | isnull(c), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Convert the DataFrame to a dictionary\n",
    "null_counts_dict = {c: null_counts.first()[c] for c in null_counts.columns}\n",
    "\n",
    "# Drop columns where more than 50% of the values are null\n",
    "df = df.drop(*[c for c, null_count in null_counts_dict.items() if null_count / total_records > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+-------+-------+-----------+--------+--------+------+------------+-----------+---------------+----+---------+---------------+--------------+-------+----------------+----------------+------------------+----------------+--------------+------------+-------------+------------+-------------+------------+------------+\n",
      "|date|time|location|    lat|    lng|county_name|district|precinct|region|subject_race|subject_sex|officer_id_hash|type|violation|citation_issued|warning_issued|outcome|contraband_found|contraband_drugs|contraband_weapons|search_conducted|search_vehicle|search_basis|vehicle_color|vehicle_make|vehicle_model|vehicle_type|vehicle_year|\n",
      "+----+----+--------+-------+-------+-----------+--------+--------+------+------------+-----------+---------------+----+---------+---------------+--------------+-------+----------------+----------------+------------------+----------------+--------------+------------+-------------+------------+-------------+------------+------------+\n",
      "|   0|   0|      91|8152359|8152288|         99|    9161|12569006|     0|         236|        251|            250|   1|     1426|              1|             1|   1426|        19293219|        19293864|          19295854|            4385|        444674|    19293327|     14873559|     7671434|      8966745|        1451|     8921042|\n",
      "+----+----+--------+-------+-------+-----------+--------+--------+------+------------+-----------+---------------+----+---------+---------------+--------------+-------+----------------+----------------+------------------+----------------+--------------+------------+-------------+------------+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Print the null counts DataFrame\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# drop column officer_id_hash\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mofficer_id_hash\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# drop column district\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistrict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# drop column officer_id_hash\n",
    "df = df.drop('officer_id_hash')\n",
    "# drop column district\n",
    "df = df.drop('district')\n",
    "# drop column region\n",
    "df = df.drop('region')\n",
    "# drop column type\n",
    "df = df.drop('type')\n",
    "# drop column citation_issued (meaningless)\n",
    "df = df.drop('citation_issued')\n",
    "# drop column warning_issued (meaningless)\n",
    "df = df.drop('warning_issued')\n",
    "\n",
    "### DROP FOR NOW OPTMIZATIONS\n",
    "\n",
    "# drop column outcome \n",
    "df = df.drop('outcome')\n",
    "# drop column vehicle_make\n",
    "df = df.drop('vehicle_make')\n",
    "# drop column vehicle_model\n",
    "df = df.drop('vehicle_model')\n",
    "# drop column vehicle_type\n",
    "df = df.drop('vehicle_type')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediary save\n",
    "df.write.format('parquet').mode('overwrite').save('../data/tx_statewide_2020_04_01-002.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset for Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 16:13:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('DataFrame Optimization') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format('parquet').load('../data/tx_statewide_2020_04_01-002.parquet')\n",
    "\n",
    "# drop column type\n",
    "df = df.drop('type')\n",
    "# drop column citation_issued (meaningless)\n",
    "df = df.drop('citation_issued')\n",
    "# drop column warning_issued (meaningless)\n",
    "df = df.drop('warning_issued')\n",
    "\n",
    "### DROP FOR NOW OPTMIZATIONS\n",
    "\n",
    "# drop column outcome\n",
    "df = df.drop('outcome')\n",
    "# drop column vehicle_make\n",
    "df = df.drop('vehicle_make')\n",
    "# drop column vehicle_model\n",
    "df = df.drop('vehicle_model')\n",
    "# drop column vehicle_type\n",
    "df = df.drop('vehicle_type')\n",
    "# drop column violation\n",
    "# df = df.drop('violation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### different values for violation and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           violation| count|\n",
      "+--------------------+------+\n",
      "|Following Too Clo...|176546|\n",
      "|Drive On Improved...|  1427|\n",
      "|Fail To Signal La...|  1164|\n",
      "|Cut In After Pass...|   354|\n",
      "|Following Too Clo...|   987|\n",
      "|Fail To Display D...|  2295|\n",
      "|Drive in Left Lan...|    26|\n",
      "|Damaged/discolore...|     2|\n",
      "|Drive/Permit to D...|    29|\n",
      "|Disregard RR Cros...|    39|\n",
      "|Wrong Side Road-n...|   413|\n",
      "|Fail To Report Ch...|   511|\n",
      "|Minor Possesses A...|   688|\n",
      "|No Valid Inspecti...|     1|\n",
      "|Speeding Over Lim...|   235|\n",
      "|Ride, Not Secured...|   304|\n",
      "|\"No/Improper Mud ...|    69|\n",
      "|Damaged/discolore...|     1|\n",
      "|Speeding Over Lim...|   150|\n",
      "|Speeding Over Lim...|    20|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show different values in type column and their counts\n",
    "df.select('violation').groupBy('violation').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### parse dataset to open in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming df is your DataFrame and 'violation' is the column to parse\n",
    "df = df.withColumn('violation', \n",
    "                   F.when(F.lower(F.col('violation')).like('%speed%'), 0)\n",
    "                    .otherwise(1))\n",
    "\n",
    "# make subject_sex 1 if 'male' and 0 if 'female'\n",
    "df = df.withColumn(\"subject_sex\", when(col(\"subject_sex\") == \"male\", 1).otherwise(0).cast(\"integer\"))\n",
    "\n",
    "# make lat and long float\n",
    "df = df.withColumn(\"lat\", col(\"lat\").cast(\"float\"))\n",
    "df = df.withColumn(\"lng\", col(\"lng\").cast(\"float\"))\n",
    "\n",
    "# make subject_race 0 if 'white', 1 if 'black', 2 if 'hispanic', 3 if 'asian', 4 if 'other' and make it an integer column\n",
    "df = df.withColumn(\"subject_race\", when(col(\"subject_race\") == \"white\", 0)\n",
    "                                   .when(col(\"subject_race\") == \"black\", 1)\n",
    "                                   .when(col(\"subject_race\") == \"hispanic\", 2)\n",
    "                                   .when(col(\"subject_race\") == \"asian\", 3)\n",
    "                                   .otherwise(4).cast(\"integer\"))\n",
    "\n",
    "\n",
    "# make search_vehicle 1 if TRUE and 0 if FALSE else NA\n",
    "df = df.withColumn(\"search_vehicle\", when(col(\"search_vehicle\") == \"TRUE\", 1)\n",
    "                                    .when(col(\"search_vehicle\") == \"FALSE\", 0)\n",
    "                                    .otherwise(None).cast(\"integer\"))\n",
    "\n",
    "# make vehicle_year an integer column and fill NA with 0\n",
    "df = df.withColumn(\"vehicle_year\", col(\"vehicle_year\").cast(\"integer\"))\n",
    "df = df.withColumn(\"vehicle_year\", when(col(\"vehicle_year\").isNull(), 0).otherwise(col(\"vehicle_year\")))\n",
    "df = df.withColumn(\"vehicle_year\", when(col(\"vehicle_year\") < 1900, 0).otherwise(col(\"vehicle_year\")))\n",
    "df = df.withColumn(\"vehicle_year\", when(col(\"vehicle_year\") > 2022, 0).otherwise(col(\"vehicle_year\")))\n",
    "\n",
    "# date column is of format 'yyyy-mm-dd' and time column is of format 'hh:mm:ss': combine them into a single timestamp column\n",
    "from pyspark.sql.functions import to_timestamp, concat_ws\n",
    "\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(concat_ws(\" \", col(\"date\"), col(\"time\")), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# drop date and time columns\n",
    "df = df.drop(\"date\", \"time\")\n",
    "\n",
    "# make search_conducted 1 if TRUE or citation and 0 if FALSE else NA and make it an integer column\n",
    "from pyspark.sql.functions import col, when, to_timestamp, concat_ws\n",
    "\n",
    "df = df.withColumn(\"search_conducted\", when((col(\"search_conducted\") == \"TRUE\") | (col(\"search_conducted\") == \"citation\"), 1)\n",
    "                                      .when(col(\"search_conducted\") == \"FALSE\", 0)\n",
    "                                      .otherwise(None).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+----------------+------------+-----------+---------+----------------+--------------+------------+-------------------+\n",
      "|            location|      lat|       lng|     county_name|subject_race|subject_sex|violation|search_conducted|search_vehicle|vehicle_year|          timestamp|\n",
      "+--------------------+---------+----------+----------------+------------+-----------+---------+----------------+--------------+------------+-------------------+\n",
      "|route: 0059, mile...|     NULL|      NULL|     Cass County|           0|          1|        0|               0|             0|        1997|2012-03-27 22:10:00|\n",
      "|route: 0020, mile...|     NULL|      NULL|   Parker County|           0|          1|        1|               0|             0|        2002|2012-03-27 22:11:00|\n",
      "|route: 0044, mile...|33.958683|-98.529686|  Wichita County|           0|          1|        1|               0|             0|        2000|2012-03-27 22:11:00|\n",
      "|route: 1788, mile...|     NULL|      NULL|  Andrews County|           2|          1|        1|               0|             0|        2011|2012-03-27 22:11:00|\n",
      "|route: 1788, mile...|     NULL|      NULL|  Andrews County|           0|          1|        1|               0|             0|        2007|2012-03-27 22:11:00|\n",
      "|route: 0059, mile...|28.993633|  -96.6152|  Jackson County|           4|          1|        1|               0|             0|        2002|2012-03-27 22:11:00|\n",
      "|route: 0035, mile...|     NULL|      NULL|     Webb County|           2|          1|        1|               0|             0|        2010|2012-03-27 22:11:00|\n",
      "|route: 0010, mile...|29.838734| -94.73535| Chambers County|           0|          1|        1|               0|             0|        2006|2012-03-27 22:12:00|\n",
      "|route: 0067, mile...|32.255566|-97.720314|Somervell County|           0|          0|        0|               0|             0|        2004|2012-03-27 22:12:00|\n",
      "|route: 0087, mile...| 35.86605| -102.0781|    Moore County|           0|          1|        0|               0|             0|           0|2012-03-27 22:13:00|\n",
      "|route: 0287, mile...|34.160767|  -99.3063|Wilbarger County|           0|          1|        0|               0|             0|        2011|2012-03-27 22:13:00|\n",
      "|route: 0082, mile...| 33.65055|-96.483765|  Grayson County|           0|          1|        0|               0|             0|        2001|2012-03-27 22:13:00|\n",
      "|route: 1001, mile...|33.194984| -94.92437|    Titus County|           0|          1|        1|               0|             0|        2008|2012-03-27 22:13:00|\n",
      "|route: 0031, mile...|32.347935| -95.26462|    Smith County|           0|          0|        0|               0|             0|        2010|2012-03-27 22:13:00|\n",
      "|route: COUNTY RD,...|29.655684|-95.658615|Fort Bend County|           2|          0|        1|               0|             0|        2001|2012-03-27 22:13:00|\n",
      "|route: 0035, mile...|     NULL|      NULL|     Bell County|           2|          0|        1|               0|             0|        2001|2012-03-27 22:13:00|\n",
      "|route: 0010, mile...| 29.53865|-98.077484|Guadalupe County|           0|          1|        1|               0|             0|        2010|2012-03-27 22:13:00|\n",
      "|route: 0287, mile...|33.544983| -97.84963| Montague County|           0|          1|        0|               0|             0|        1999|2012-03-27 22:14:00|\n",
      "|route: CITY ST, m...|     NULL|      NULL|Jefferson County|           0|          1|        1|               0|             0|        1999|2012-03-27 22:49:00|\n",
      "|route: 0281, mile...|33.823566| -98.48548|   Archer County|           2|          1|        1|               0|             0|        2003|2012-03-27 22:50:00|\n",
      "+--------------------+---------+----------+----------------+------------+-----------+---------+----------------+--------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('location', 'string'),\n",
       " ('lat', 'float'),\n",
       " ('lng', 'float'),\n",
       " ('county_name', 'string'),\n",
       " ('subject_race', 'int'),\n",
       " ('subject_sex', 'int'),\n",
       " ('violation', 'int'),\n",
       " ('search_conducted', 'int'),\n",
       " ('search_vehicle', 'int'),\n",
       " ('vehicle_year', 'int'),\n",
       " ('timestamp', 'timestamp')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/05/10 16:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "[Stage 31:>                                                       (0 + 16) / 18]\n",
      "Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"refresh progress\"\n",
      "\n",
      "Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"RemoteBlock-temp-file-clean-thread\"\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1362)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 WARN Utils: Suppressing exception in catch: Java heap space\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromBytes(UTF8String.java:109)\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.toLowerCase(UTF8String.java:410)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda/0x000076e770193ac8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda/0x000076e77013c978.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000076e76fd7fd90.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000076e76f9092d0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.runWith(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects\n",
      "24/05/10 16:16:13 ERROR Executor: Exception in task 1.0 in stage 31.0 (TID 178)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Executor: Exception in task 11.0 in stage 31.0 (TID 188)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Executor: Exception in task 7.0 in stage 31.0 (TID 184)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.parquet.io.api.Binary$ByteBufferBackedBinary.getBytes(Binary.java:455)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToBinary(ParquetDictionary.java:73)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.getUTF8String(WritableColumnVector.java:433)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda/0x000076e770193ac8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda/0x000076e77013c978.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000076e76fd7fd90.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000076e76f9092d0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.runWith(Unknown Source)\n",
      "\t... 1 more\n",
      "24/05/10 16:16:13 ERROR Executor: Exception in task 8.0 in stage 31.0 (TID 185)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Executor: Exception in task 5.0 in stage 31.0 (TID 182)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Executor: Exception in task 15.0 in stage 31.0 (TID 192)\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#440,Executor task launch worker for task 11.0 in stage 31.0 (TID 188),5,main]\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#437,Executor task launch worker for task 7.0 in stage 31.0 (TID 184),5,main]\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.parquet.io.api.Binary$ByteBufferBackedBinary.getBytes(Binary.java:455)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToBinary(ParquetDictionary.java:73)\n",
      "\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.getUTF8String(WritableColumnVector.java:433)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda/0x000076e770193ac8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec$$Lambda/0x000076e77013c978.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda/0x000076e76fd7fd90.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda/0x000076e76f9092d0.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.runWith(Unknown Source)\n",
      "\t... 1 more\n",
      "24/05/10 16:16:13 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#432,Executor task launch worker for task 8.0 in stage 31.0 (TID 185),5,main]\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#447,Executor task launch worker for task 1.0 in stage 31.0 (TID 178),5,main]\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#449,Executor task launch worker for task 15.0 in stage 31.0 (TID 192),5,main]\n",
      "org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#439,Executor task launch worker for task 5.0 in stage 31.0 (TID 182),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:13 WARN TaskSetManager: Lost task 11.0 in stage 31.0 (TID 188) (lux executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/05/10 16:16:13 ERROR TaskSetManager: Task 11 in stage 31.0 failed 1 times; aborting job\n",
      "24/05/10 16:16:13 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2b64ce54 rejected from java.util.concurrent.ThreadPoolExecutor@4f8c18f2[Shutting down, pool size = 12, active threads = 12, queued tasks = 0, completed tasks = 181]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:14 WARN TaskSetManager: Lost task 5.0 in stage 31.0 (TID 182) (lux executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Aborting job e266705c-b2e1-449a-8abe-68ef85e5e106.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 31.0 failed 1 times, most recent failure: Lost task 11.0 in stage 31.0 (TID 188) (lux executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "24/05/10 16:16:14 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:128)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000012_189\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000014_191\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000004_181\n",
      "24/05/10 16:16:14 WARN TaskSetManager: Lost task 12.0 in stage 31.0 (TID 189) (lux executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 11 in stage 31.0 failed 1 times, most recent failure: Lost task 11.0 in stage 31.0 (TID 188) (lux executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 WARN TaskSetManager: Lost task 14.0 in stage 31.0 (TID 191) (lux executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 11 in stage 31.0 failed 1 times, most recent failure: Lost task 11.0 in stage 31.0 (TID 188) (lux executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000010_187\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/05/10 16:16:14 WARN TaskSetManager: Lost task 4.0 in stage 31.0 (TID 181) (lux executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 11 in stage 31.0 failed 1 times, most recent failure: Lost task 11.0 in stage 31.0 (TID 188) (lux executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000002_179\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000009_186\n",
      "24/05/10 16:16:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000013_190\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x000076e770382d20@7ccd9e2a rejected from java.util.concurrent.ThreadPoolExecutor@430dcd78[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 186]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:14 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x000076e770382d20@22fa797f rejected from java.util.concurrent.ThreadPoolExecutor@430dcd78[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 186]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000006_183\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000000_177\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "24/05/10 16:16:14 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x000076e770382d20@62ceb826 rejected from java.util.concurrent.ThreadPoolExecutor@430dcd78[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 186]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:14 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x000076e770382d20@2273cc76 rejected from java.util.concurrent.ThreadPoolExecutor@430dcd78[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 186]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:14 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x000076e770382d20@33c5dc66 rejected from java.util.concurrent.ThreadPoolExecutor@430dcd78[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 186]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "24/05/10 16:16:14 WARN FileOutputCommitter: Could not delete file:/home/lux/Documents/Criminalysis/data/tx_statewide_2020_04_01-002_clean.parquet/_temporary/0/_temporary/attempt_20240510161402215304570356016005_0031_m_000003_180\n",
      "24/05/10 16:16:14 ERROR FileFormatWriter: Job job_20240510161402215304570356016005_0031 aborted.\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# intermediary save\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/tx_statewide_2020_04_01-002_clean.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:136\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.streaming.StreamingQueryException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StreamingQueryException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.execution.QueryExecutionException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryExecutionException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Order matters. NumberFormatException inherits IllegalArgumentException.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lux/.pyenv/versions/3.11.6/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# intermediary save\n",
    "df.write.format('parquet').mode('overwrite').save('../data/tx_statewide_2020_04_01-002_clean.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 4.0.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow>=4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for Pandas VIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 16:09:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"parsing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Read the parquet file\n",
    "df = spark.read.format('parquet').load('../data/tx_statewide_2020_04_01-002_clean.parquet')\n",
    "\n",
    "# remove the file if it exists\n",
    "if os.path.exists('../data/tx_statewide_2020_04_01-002_clean.csv'):\n",
    "    os.remove('../data/tx_statewide_2020_04_01-002_clean.csv')\n",
    "\n",
    "# Function to append a Pandas DataFrame to a CSV file\n",
    "def append_to_csv(pandas_df, filename, header=True, index=False):\n",
    "    pandas_df.to_csv(filename, mode='a', header=header, index=index)\n",
    "\n",
    "# Adjusted function to accept column names\n",
    "def write_partition_to_csv(column_names, iterator):\n",
    "    pandas_df = pd.DataFrame(list(iterator), columns=column_names)\n",
    "    if not pandas_df.empty:\n",
    "        append_to_csv(pandas_df, '../data/tx_statewide_2020_04_01-002_clean.csv', header=not os.path.exists('../data/tx_statewide_2020_04_01-002_clean.csv'), index=False)\n",
    "\n",
    "# Capture column names outside the function\n",
    "column_names = df.columns\n",
    "\n",
    "# Use partial to pass column names along with the iterator\n",
    "df.foreachPartition(partial(write_partition_to_csv, column_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/10 16:48:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------------+----------------+------------+-----------+---------+----------------+--------------+------------+-------------------+\n",
      "|            location|               lat|                lng|     county_name|subject_race|subject_sex|violation|search_conducted|search_vehicle|vehicle_year|          timestamp|\n",
      "+--------------------+------------------+-------------------+----------------+------------+-----------+---------+----------------+--------------+------------+-------------------+\n",
      "|route: 0010, mile...|29.650999069213867| -97.51920318603516| Gonzales County|           4|          1|        0|               0|           0.0|           0|2008-08-12 13:46:00|\n",
      "|route: 0010, mile...| 29.65060043334961| -97.50606536865234| Gonzales County|           4|          1|        1|               0|           0.0|           0|2008-08-12 13:46:00|\n",
      "|route: 0271, mile...|33.733150482177734| -95.54741668701172|    Lamar County|           0|          1|        1|               0|           0.0|           0|2008-08-12 13:46:00|\n",
      "|route: 0010, mile...|              NULL|               NULL|   Harris County|           0|          0|        1|               0|           0.0|           0|2008-08-12 13:46:00|\n",
      "|route: 0303, mile...| 33.46066665649414|-102.48912048339844|  Hockley County|           4|          1|        0|               0|           0.0|           0|2008-08-12 13:47:00|\n",
      "|route: 0082, mile...| 33.78779983520508| -97.71025085449219| Montague County|           0|          0|        0|               0|           0.0|           0|2008-08-12 13:47:00|\n",
      "|route: 0281, mile...|27.578716278076172| -98.09376525878906|Jim Wells County|           0|          0|        0|               0|           0.0|           0|2008-08-12 13:47:00|\n",
      "|route: 0277, mile...| 29.29083251953125|-100.77936553955078|Val Verde County|           0|          0|        1|               0|           0.0|           0|2008-08-12 13:47:00|\n",
      "|route: 0287, mile...|36.265350341796875|-102.04633331298828|  Sherman County|           0|          0|        0|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|       route: DENNIS| 32.66663360595703| -97.90460205078125|   Parker County|           0|          1|        0|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|route: 0083, mile...| 28.16946792602539|  -99.6010513305664|     Webb County|           0|          0|        1|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|route: 0010, mile...|  30.8975830078125|-103.00325012207031|    Pecos County|           0|          1|        0|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|route: 0191, mile...|31.887165069580078|-102.36421966552734|    Ector County|           0|          0|        1|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|route: 0010, mile...|29.803800582885742| -94.98639678955078|   Harris County|           1|          1|        0|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|route: 0083, mile...|  32.2486686706543| -99.75965118408203|   Taylor County|           0|          1|        1|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|route: 0146, mile...|              NULL|               NULL|Galveston County|           2|          1|        1|               0|           0.0|           0|2008-08-12 13:48:00|\n",
      "|route: 0037, mile...|29.377683639526367| -98.46246337890625|    Bexar County|           0|          1|        1|               0|           0.0|           0|2008-08-12 13:49:00|\n",
      "|route: 0030, mile...|32.893531799316406| -96.47896575927734| Rockwall County|           0|          1|        0|               0|           0.0|           0|2008-08-12 13:49:00|\n",
      "|route: 0287, mile...|              NULL|               NULL|    Ellis County|           0|          1|        1|               0|           0.0|           0|2008-08-12 13:49:00|\n",
      "|route: 0020, mile...|32.279666900634766|    -101.3583984375|   Howard County|           0|          0|        0|               0|           0.0|           0|2008-08-12 13:50:00|\n",
      "+--------------------+------------------+-------------------+----------------+------------+-----------+---------+----------------+--------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"parsing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Read the csv file\n",
    "df = spark.read.format('csv').option('header', 'true').load('../data/tx_statewide_2020_04_01-002_clean.csv')\n",
    "\n",
    "# show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for Pandas ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 16:48:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "# remove the file if it exists\n",
    "if os.path.exists('../data/tx_statewide_2020_04_01-002_clean_ml.csv'):\n",
    "    os.remove('../data/tx_statewide_2020_04_01-002_clean_ml.csv')\n",
    "\n",
    "# Function to append a Pandas DataFrame to a CSV file\n",
    "def append_to_csv(pandas_df, filename, header=True, index=False):\n",
    "    pandas_df.to_csv(filename, mode='a', header=header, index=index)\n",
    "\n",
    "# Adjusted function to accept column names\n",
    "def write_partition_to_csv(column_names, iterator):\n",
    "    pandas_df = pd.DataFrame(list(iterator), columns=column_names)\n",
    "    if not pandas_df.empty:\n",
    "        append_to_csv(pandas_df, '../data/tx_statewide_2020_04_01-002_clean_ml.csv', header=not os.path.exists('../data/tx_statewide_2020_04_01-002_clean_ml.csv'), index=False)\n",
    "\n",
    "# Capture column names outside the function\n",
    "column_names = df.columns\n",
    "\n",
    "# Use partial to pass column names along with the iterator\n",
    "df.foreachPartition(partial(write_partition_to_csv, column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
